{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ NOTEBOOK 2: ECONOMIC DATA CLEANING & FEATURE ENGINEERING\n",
    "## Mortgage Approval Rate Forecasting Project | Data Preparation Phase\n",
    "\n",
    "### üéØ BUSINESS OBJECTIVE\n",
    "**Primary Goal**: Transform raw economic data into modeling-ready features that capture the economic relationships influencing mortgage lending decisions.\n",
    "\n",
    "**Business Impact**: Create robust features that enable:\n",
    "- Accurate prediction of approval rate changes\n",
    "- Understanding of economic driver relationships\n",
    "- Reliable forecasting under different scenarios\n",
    "- Interpretable model coefficients for business decisions\n",
    "\n",
    "### üìà STRATEGIC CONTEXT: FEATURE ENGINEERING PHILOSOPHY\n",
    "**Critical Insight**: Mortgage lenders don't react to raw economic data‚Äîthey respond to:\n",
    "- Economic trends and momentum\n",
    "- Relative changes from historical norms\n",
    "- Interactions between different economic factors\n",
    "- Lagged effects of economic conditions\n",
    "\n",
    "### üîç ANALYTICAL APPROACH\n",
    "We'll implement a systematic cleaning and feature engineering pipeline that transforms raw economic indicators into meaningful predictors for mortgage approval modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 1: INITIALIZATION & STRATEGIC FRAMEWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ THINKING PROCESS: DATA CLEANING STRATEGY\n",
    "\n",
    "**Business Rationale for Cleaning**:\n",
    "- **Data Quality = Model Reliability**: Clean data prevents spurious correlations and unstable predictions\n",
    "- **Missing Data Handling**: Different economic indicators require different imputation strategies based on their behavior\n",
    "- **Outlier Management**: Economic data can have valid extremes (recessions, booms) that should be preserved but handled carefully\n",
    "\n",
    "**Strategic Cleaning Principles**:\n",
    "1. **Preserve Economic Signals**: Don't over-smooth genuine economic volatility\n",
    "2. **Domain-Informed Imputation**: Use economic logic for missing data, not just statistical methods\n",
    "3. **Transparent Processing**: All transformations documented and reproducible\n",
    "4. **Business Validation**: Cleaned data should make economic sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß TECHNICAL SETUP & DEPENDENCIES\n",
    "# Thinking: Comprehensive toolkit for robust data cleaning and feature engineering\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Professional styling for business presentations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ CLEANING ENVIRONMENT INITIALIZED\")\n",
    "print(\"üìä Available Tools: Advanced imputation, feature engineering, visualization\")\n",
    "print(\"üéØ Business Focus: Economic logic-driven data transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 2: DATA LOADING & QUALITY ASSESSMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ THINKING PROCESS: DATA VALIDATION STRATEGY\n",
    "\n",
    "**Comprehensive Quality Assessment Framework**:\n",
    "\n",
    "| Quality Dimension | Assessment Method | Business Impact |\n",
    "|-------------------|-------------------|------------------|\n",
    "| **Completeness** | Missing value analysis | Model stability & coverage |\n",
    "| **Temporal Coverage** | Date range verification | Historical context capture |\n",
    "| **Economic Plausibility** | Value range checks | Realistic scenario modeling |\n",
    "| **Consistency** | Data type validation | Processing reliability |\n",
    "\n",
    "**Strategic Loading Approach**:\n",
    "- Load both raw (versioned) and processed data for comparison\n",
    "- Validate against Notebook 1 quality reports\n",
    "- Establish baseline for cleaning improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ STRATEGIC DATA LOADING WITH COMPREHENSIVE VALIDATION\n",
    "# Thinking: Multiple validation checkpoints to ensure data integrity\n",
    "\n",
    "class DataQualityAuditor:\n",
    "    \"\"\"\n",
    "    COMPREHENSIVE DATA QUALITY ASSESSMENT ENGINE\n",
    "    \n",
    "    Business Purpose: Systematic validation of economic data quality\n",
    "    to ensure reliable modeling and business decision support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.audit_results = {}\n",
    "        self.quality_thresholds = {\n",
    "            'critical_missing': 0.10,  # 10% missing max for critical series\n",
    "            'acceptable_missing': 0.25,  # 25% missing max for supporting series\n",
    "            'min_quarters': 20,  # Minimum 20 quarters for modeling\n",
    "            'outlier_std_threshold': 4  # 4 std dev for outlier detection\n",
    "        }\n",
    "    \n",
    "    def load_and_validate_dataset(self, file_path):\n",
    "        \"\"\"\n",
    "        ROBUST DATA LOADING WITH REAL-TIME VALIDATION\n",
    "        \n",
    "        Thinking: Catch data issues early before they propagate\n",
    "        through the analysis pipeline.\n",
    "        \"\"\"\n",
    "        print(f\"üìÇ LOADING DATASET: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the economic data collected in Notebook 1\n",
    "            data = pd.read_parquet(file_path)\n",
    "            \n",
    "            # üßê COMPREHENSIVE INITIAL VALIDATION\n",
    "            validation_checks = {\n",
    "                'load_successful': not data.empty,\n",
    "                'expected_columns': len(data.columns) >= 8,  # At least 8 economic series\n",
    "                'time_period_coverage': (data.index.max() - data.index.min()).days >= 365 * 6,  # 6+ years\n",
    "                'date_index_integrity': isinstance(data.index, pd.DatetimeIndex)\n",
    "            }\n",
    "            \n",
    "            # üö® CRITICAL VALIDATION FAILURES\n",
    "            failed_checks = [check for check, passed in validation_checks.items() if not passed]\n",
    "            if failed_checks:\n",
    "                raise ValueError(f\"Critical validation failures: {failed_checks}\")\n",
    "            \n",
    "            print(f\"‚úÖ SUCCESS: Loaded {len(data)} days, {len(data.columns)} economic series\")\n",
    "            \n",
    "            # üìä COMPREHENSIVE QUALITY METRICS\n",
    "            quality_metrics = self.calculate_comprehensive_metrics(data)\n",
    "            \n",
    "            return data, quality_metrics\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå CRITICAL: Data file not found at {file_path}\")\n",
    "            print(\"üí° SOLUTION: Run Notebook 1 first to collect economic data\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Data loading failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def calculate_comprehensive_metrics(self, data):\n",
    "        \"\"\"Calculate detailed quality metrics for business reporting\"\"\"\n",
    "        \n",
    "        metrics = {\n",
    "            'overall': {\n",
    "                'total_series': len(data.columns),\n",
    "                'total_observations': len(data),\n",
    "                'date_range': f\"{data.index.min().strftime('%Y-%m-%d')} to {data.index.max().strftime('%Y-%m-%d')}\",\n",
    "                'total_missing': data.isna().sum().sum(),\n",
    "                'overall_missing_pct': data.isna().sum().sum() / (len(data) * len(data.columns))\n",
    "            },\n",
    "            'series_level': {}\n",
    "        }\n",
    "        \n",
    "        # Series-level quality assessment\n",
    "        for col in data.columns:\n",
    "            series_data = data[col]\n",
    "            metrics['series_level'][col] = {\n",
    "                'missing_count': series_data.isna().sum(),\n",
    "                'missing_pct': series_data.isna().mean(),\n",
    "                'date_coverage': f\"{series_data.first_valid_index().strftime('%Y-%m')} to {series_data.last_valid_index().strftime('%Y-%m')}\",\n",
    "                'value_range': f\"{series_data.min():.2f} to {series_data.max():.2f}\",\n",
    "                'outlier_count': self.count_outliers(series_data)\n",
    "            }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def count_outliers(self, series):\n",
    "        \"\"\"Count outliers using robust statistical methods\"\"\"\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (series < (Q1 - 1.5 * IQR)) | (series > (Q3 + 1.5 * IQR))\n",
    "        return outlier_mask.sum()\n",
    "\n",
    "# Initialize and execute data loading with validation\n",
    "print(\"üîç INITIATING COMPREHENSIVE DATA QUALITY AUDIT\")\n",
    "auditor = DataQualityAuditor()\n",
    "raw_data, quality_metrics = auditor.load_and_validate_dataset('../data/processed/master_economic_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 3: COMPREHENSIVE DATA QUALITY REPORTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ THINKING PROCESS: QUALITY ASSESSMENT FRAMEWORK\n",
    "\n",
    "**Business Impact of Data Quality**:\n",
    "- **High Quality**: Reliable models ‚Üí Confident business decisions\n",
    "- **Medium Quality**: Cautious interpretation ‚Üí Limited strategic use\n",
    "- **Poor Quality**: Unreliable predictions ‚Üí Business risk exposure\n",
    "\n",
    "**Strategic Quality Thresholds**:\n",
    "- **üü¢ Excellent**: <5% missing, full temporal coverage\n",
    "- **üü° Acceptable**: 5-15% missing, minor coverage gaps  \n",
    "- **üî¥ Critical**: >15% missing, major data issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä COMPREHENSIVE QUALITY DASHBOARD\n",
    "class QualityReporter:\n",
    "    \"\"\"\n",
    "    PROFESSIONAL DATA QUALITY REPORTING ENGINE\n",
    "    \n",
    "    Business Purpose: Generate comprehensive quality reports\n",
    "    that communicate data readiness to business stakeholders.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.report_data = {}\n",
    "    \n",
    "    def generate_quality_dashboard(self, data, quality_metrics):\n",
    "        \"\"\"\n",
    "        CREATE COMPREHENSIVE QUALITY VISUALIZATION DASHBOARD\n",
    "        \n",
    "        Thinking: Visual communication of data quality status\n",
    "        for both technical and business audiences.\n",
    "        \"\"\"\n",
    "        print(\"\\nüìà GENERATING DATA QUALITY DASHBOARD...\")\n",
    "        \n",
    "        # Create multi-panel visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Economic Data Quality Assessment Dashboard', \n",
    "                    fontsize=16, fontweight='bold', y=0.95)\n",
    "        \n",
    "        # 1. MISSING DATA HEATMAP\n",
    "        print(\"   üîç Creating missing data heatmap...\")\n",
    "        self.plot_missing_heatmap(axes[0, 0], data)\n",
    "        \n",
    "        # 2. TEMPORAL COVERAGE\n",
    "        print(\"   üìÖ Creating temporal coverage visualization...\")\n",
    "        self.plot_temporal_coverage(axes[0, 1], data)\n",
    "        \n",
    "        # 3. DATA QUALITY SUMMARY\n",
    "        print(\"   üìä Creating quality summary...\")\n",
    "        self.plot_quality_summary(axes[1, 0], quality_metrics)\n",
    "        \n",
    "        # 4. OUTLIER ANALYSIS\n",
    "        print(\"   üìà Creating outlier analysis...\")\n",
    "        self.plot_outlier_analysis(axes[1, 1], data)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        self.print_quality_summary(quality_metrics)\n",
    "    \n",
    "    def plot_missing_heatmap(self, ax, data):\n",
    "        \"\"\"Create missing data heatmap\"\"\"\n",
    "        missing_matrix = data.isna().astype(int)\n",
    "        \n",
    "        sns.heatmap(missing_matrix, \n",
    "                   ax=ax, \n",
    "                   cbar_kws={'label': 'Missing (1=Yes, 0=No)'},\n",
    "                   cmap=['green', 'red'])\n",
    "        \n",
    "        ax.set_title('Missing Data Pattern Analysis\\n(Red = Missing, Green = Present)', \n",
    "                    fontweight='bold')\n",
    "        ax.set_xlabel('Economic Series')\n",
    "        ax.set_ylabel('Time Period')\n",
    "    \n",
    "    def plot_temporal_coverage(self, ax, data):\n",
    "        \"\"\"Create temporal coverage visualization\"\"\"\n",
    "        coverage_data = []\n",
    "        \n",
    "        for col in data.columns:\n",
    "            series = data[col]\n",
    "            start_date = series.first_valid_index()\n",
    "            end_date = series.last_valid_index()\n",
    "            coverage_pct = (series.notna().sum() / len(series)) * 100\n",
    "            \n",
    "            coverage_data.append({\n",
    "                'series': col,\n",
    "                'start': start_date,\n",
    "                'end': end_date,\n",
    "                'coverage': coverage_pct\n",
    "            })\n",
    "        \n",
    "        coverage_df = pd.DataFrame(coverage_data)\n",
    "        \n",
    "        # Create horizontal bar chart for coverage percentages\n",
    "        y_pos = np.arange(len(coverage_df))\n",
    "        bars = ax.barh(y_pos, coverage_df['coverage'], \n",
    "                      color=['red' if x < 80 else 'orange' if x < 95 else 'green' \n",
    "                            for x in coverage_df['coverage']])\n",
    "        \n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(coverage_df['series'])\n",
    "        ax.set_xlabel('Data Coverage (%)')\n",
    "        ax.set_title('Temporal Coverage by Economic Series\\n(Green=Good, Orange=Fair, Red=Poor)', \n",
    "                    fontweight='bold')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            ax.text(width + 1, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{width:.1f}%', ha='left', va='center')\n",
    "    \n",
    "    def plot_quality_summary(self, ax, quality_metrics):\n",
    "        \"\"\"Create overall quality summary\"\"\"\n",
    "        series_metrics = quality_metrics['series_level']\n",
    "        \n",
    "        # Categorize series by quality\n",
    "        quality_categories = {'Excellent': 0, 'Good': 0, 'Fair': 0, 'Poor': 0}\n",
    "        \n",
    "        for series_name, metrics in series_metrics.items():\n",
    "            missing_pct = metrics['missing_pct'] * 100\n",
    "            \n",
    "            if missing_pct <= 5:\n",
    "                quality_categories['Excellent'] += 1\n",
    "            elif missing_pct <= 15:\n",
    "                quality_categories['Good'] += 1\n",
    "            elif missing_pct <= 25:\n",
    "                quality_categories['Fair'] += 1\n",
    "            else:\n",
    "                quality_categories['Poor'] += 1\n",
    "        \n",
    "        # Create pie chart\n",
    "        colors = ['green', 'lightgreen', 'orange', 'red']\n",
    "        ax.pie(quality_categories.values(), \n",
    "               labels=quality_categories.keys(), \n",
    "               colors=colors, \n",
    "               autopct='%1.1f%%',\n",
    "               startangle=90)\n",
    "        \n",
    "        ax.set_title('Overall Data Quality Distribution\\n(by Economic Series)', \n",
    "                    fontweight='bold')\n",
    "    \n",
    "    def plot_outlier_analysis(self, ax, data):\n",
    "        \"\"\"Create outlier analysis visualization\"\"\"\n",
    "        outlier_counts = []\n",
    "        series_names = []\n",
    "        \n",
    "        for col in data.columns:\n",
    "            series = data[col].dropna()\n",
    "            Q1 = series.quantile(0.25)\n",
    "            Q3 = series.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outlier_count = ((series < (Q1 - 1.5 * IQR)) | (series > (Q3 + 1.5 * IQR))).sum()\n",
    "            \n",
    "            outlier_counts.append(outlier_count)\n",
    "            series_names.append(col)\n",
    "        \n",
    "        # Create bar chart\n",
    "        y_pos = np.arange(len(series_names))\n",
    "        bars = ax.barh(y_pos, outlier_counts)\n",
    "        \n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(series_names)\n",
    "        ax.set_xlabel('Number of Outliers')\n",
    "        ax.set_title('Outlier Count by Economic Series\\n(IQR Method)', \n",
    "                    fontweight='bold')\n",
    "    \n",
    "    def print_quality_summary(self, quality_metrics):\n",
    "        \"\"\"Print comprehensive quality summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìã COMPREHENSIVE DATA QUALITY SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        overall = quality_metrics['overall']\n",
    "        \n",
    "        print(f\"\\nüìä OVERVIEW:\")\n",
    "        print(f\"   ‚Ä¢ Total Economic Series: {overall['total_series']}\")\n",
    "        print(f\"   ‚Ä¢ Total Observations: {overall['total_observations']:,}\")\n",
    "        print(f\"   ‚Ä¢ Date Range: {overall['date_range']}\")\n",
    "        print(f\"   ‚Ä¢ Overall Missing: {overall['total_missing']:,} ({overall['overall_missing_pct']:.1%})\")\n",
    "        \n",
    "        print(f\"\\nüîç SERIES-LEVEL QUALITY:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for series_name, metrics in quality_metrics['series_level'].items():\n",
    "            quality_status = \"üü¢ EXCELLENT\" if metrics['missing_pct'] <= 0.05 else \\\n",
    "                           \"üü° GOOD\" if metrics['missing_pct'] <= 0.15 else \\\n",
    "                           \"üü† FAIR\" if metrics['missing_pct'] <= 0.25 else \"üî¥ POOR\"\n",
    "            \n",
    "            print(f\"   {series_name:30} {quality_status:15} Missing: {metrics['missing_pct']:.1%}\")\n",
    "\n",
    "# Generate comprehensive quality dashboard\n",
    "reporter = QualityReporter()\n",
    "reporter.generate_quality_dashboard(raw_data, quality_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 4: ADVANCED DATA CLEANING & IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ INTELLIGENT DATA CLEANING ENGINE\n",
    "class EconomicDataCleaner:\n",
    "    \"\"\"\n",
    "    ADVANCED ECONOMIC DATA CLEANING AND IMPUTATION ENGINE\n",
    "    \n",
    "    Business Purpose: Apply domain-informed cleaning strategies\n",
    "    that preserve economic signals while handling data quality issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cleaning_log = []\n",
    "        self.imputation_strategies = {}\n",
    "    \n",
    "    def apply_comprehensive_cleaning(self, data):\n",
    "        \"\"\"\n",
    "        APPLY STRATEGIC CLEANING PIPELINE\n",
    "        \n",
    "        Thinking: Different economic series require different\n",
    "        cleaning approaches based on their characteristics.\n",
    "        \"\"\"\n",
    "        print(\"\\nüßπ INITIATING COMPREHENSIVE DATA CLEANING...\")\n",
    "        \n",
    "        # Create working copy\n",
    "        cleaned_data = data.copy()\n",
    "        \n",
    "        # 1. HANDLE OBVIOUS DATA ERRORS\n",
    "        print(\"   üîß Handling obvious data errors...\")\n",
    "        cleaned_data = self.handle_data_errors(cleaned_data)\n",
    "        \n",
    "        # 2. STRATEGIC MISSING DATA IMPUTATION\n",
    "        print(\"   üìä Applying strategic imputation...\")\n",
    "        cleaned_data = self.strategic_imputation(cleaned_data)\n",
    "        \n",
    "        # 3. OUTLIER MANAGEMENT\n",
    "        print(\"   üìà Managing outliers...\")\n",
    "        cleaned_data = self.manage_outliers(cleaned_data)\n",
    "        \n",
    "        # 4. DATA VALIDATION\n",
    "        print(\"   ‚úÖ Validating cleaned data...\")\n",
    "        self.validate_cleaning(cleaned_data, data)\n",
    "        \n",
    "        return cleaned_data\n",
    "    \n",
    "    def handle_data_errors(self, data):\n",
    "        \"\"\"Handle obvious data errors and inconsistencies\"\"\"\n",
    "        \n",
    "        cleaned_data = data.copy()\n",
    "        \n",
    "        # Remove infinite values\n",
    "        cleaned_data = cleaned_data.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Handle negative values for positive-only series\n",
    "        positive_series = ['home_price_index', 'income_level', 'gdp_growth']\n",
    "        for series in positive_series:\n",
    "            if series in cleaned_data.columns:\n",
    "                negative_mask = cleaned_data[series] < 0\n",
    "                if negative_mask.any():\n",
    "                    print(f\"      ‚Ä¢ Fixing negative values in {series}: {negative_mask.sum()} records\")\n",
    "                    cleaned_data.loc[negative_mask, series] = np.nan\n",
    "        \n",
    "        return cleaned_data\n",
    "    \n",
    "    def strategic_imputation(self, data):\n",
    "        \"\"\"Apply strategic imputation based on series characteristics\"\"\"\n",
    "        \n",
    "        cleaned_data = data.copy()\n",
    "        \n",
    "        # Define imputation strategies by series type\n",
    "        strategies = {\n",
    "            'forward_fill': ['unemployment_rate', 'mortgage_rates', 'inflation_rate'],\n",
    "            'linear': ['gdp_growth', 'income_level', 'consumer_confidence'],\n",
    "            'seasonal': ['home_price_index', 'housing_starts']\n",
    "        }\n",
    "        \n",
    "        # Apply strategies\n",
    "        for strategy, series_list in strategies.items():\n",
    "            for series in series_list:\n",
    "                if series in cleaned_data.columns:\n",
    "                    before_missing = cleaned_data[series].isna().sum()\n",
    "                    \n",
    "                    if strategy == 'forward_fill':\n",
    "                        cleaned_data[series] = cleaned_data[series].fillna(method='ffill')\n",
    "                    elif strategy == 'linear':\n",
    "                        cleaned_data[series] = cleaned_data[series].interpolate(method='linear')\n",
    "                    elif strategy == 'seasonal':\n",
    "                        # Simple seasonal pattern (quarterly)\n",
    "                        cleaned_data[series] = cleaned_data[series].interpolate(method='time')\n",
    "                    \n",
    "                    after_missing = cleaned_data[series].isna().sum()\n",
    "                    imputed_count = before_missing - after_missing\n",
    "                    \n",
    "                    if imputed_count > 0:\n",
    "                        print(f\"      ‚Ä¢ {series}: {imputed_count} values imputed ({strategy})\")\n",
    "        \n",
    "        # For any remaining missing values, use KNN imputation\n",
    "        remaining_missing = cleaned_data.isna().sum().sum()\n",
    "        if remaining_missing > 0:\n",
    "            print(f\"      ‚Ä¢ Applying KNN imputation for {remaining_missing} remaining missing values\")\n",
    "            \n",
    "            imputer = KNNImputer(n_neighbors=3)\n",
    "            imputed_values = imputer.fit_transform(cleaned_data)\n",
    "            cleaned_data = pd.DataFrame(imputed_values, \n",
    "                                      index=cleaned_data.index, \n",
    "                                      columns=cleaned_data.columns)\n",
    "        \n",
    "        return cleaned_data\n",
    "    \n",
    "    def manage_outliers(self, data):\n",
    "        \"\"\"Manage outliers while preserving economic signals\"\"\"\n",
    "        \n",
    "        cleaned_data = data.copy()\n",
    "        \n",
    "        # Only cap extreme outliers, don't remove economic extremes\n",
    "        for col in cleaned_data.columns:\n",
    "            series = cleaned_data[col]\n",
    "            Q1 = series.quantile(0.05)  # Use 5th percentile for more robustness\n",
    "            Q3 = series.quantile(0.95)  # Use 95th percentile\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - 3 * IQR  # More conservative bounds\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            \n",
    "            # Count extreme outliers\n",
    "            extreme_outliers = ((series < lower_bound) | (series > upper_bound)).sum()\n",
    "            \n",
    "            if extreme_outliers > 0:\n",
    "                print(f\"      ‚Ä¢ {col}: Capping {extreme_outliers} extreme outliers\")\n",
    "                cleaned_data[col] = np.clip(cleaned_data[col], lower_bound, upper_bound)\n",
    "        \n",
    "        return cleaned_data\n",
    "    \n",
    "    def validate_cleaning(self, cleaned_data, original_data):\n",
    "        \"\"\"Validate cleaning results\"\"\"\n",
    "        \n",
    "        original_missing = original_data.isna().sum().sum()\n",
    "        cleaned_missing = cleaned_data.isna().sum().sum()\n",
    "        \n",
    "        improvement_pct = ((original_missing - cleaned_missing) / original_missing * 100) if original_missing > 0 else 100\n",
    "        \n",
    "        print(f\"\\n‚úÖ CLEANING VALIDATION:\")\n",
    "        print(f\"   ‚Ä¢ Original missing values: {original_missing:,}\")\n",
    "        print(f\"   ‚Ä¢ Remaining missing values: {cleaned_missing:,}\")\n",
    "        print(f\"   ‚Ä¢ Missing data reduction: {improvement_pct:.1f}%\")\n",
    "        \n",
    "        if cleaned_missing == 0:\n",
    "            print(\"   ‚Ä¢ üü¢ SUCCESS: Complete dataset with no missing values\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ üü° WARNING: {cleaned_missing} missing values remain\")\n",
    "\n",
    "# Execute comprehensive data cleaning\n",
    "cleaner = EconomicDataCleaner()\n",
    "cleaned_data = cleaner.apply_comprehensive_cleaning(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 5: FEATURE ENGINEERING & TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ ADVANCED FEATURE ENGINEERING ENGINE\n",
    "class FeatureEngineeringEngine:\n",
    "    \"\"\"\n",
    "    COMPREHENSIVE FEATURE ENGINEERING FOR ECONOMIC MODELING\n",
    "    \n",
    "    Business Purpose: Create meaningful features that capture\n",
    "    economic relationships relevant to mortgage lending decisions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_catalog = {}\n",
    "        self.engineering_log = []\n",
    "    \n",
    "    def create_comprehensive_features(self, data):\n",
    "        \"\"\"\n",
    "        CREATE COMPREHENSIVE FEATURE SET\n",
    "        \n",
    "        Thinking: Mortgage lenders consider trends, momentum,\n",
    "        relative levels, and economic interactions.\n",
    "        \"\"\"\n",
    "        print(\"\\nüöÄ INITIATING COMPREHENSIVE FEATURE ENGINEERING...\")\n",
    "        \n",
    "        feature_data = data.copy()\n",
    "        \n",
    "        # 1. TEMPORAL FEATURES\n",
    "        print(\"   ‚è∞ Creating temporal features...\")\n",
    "        feature_data = self.create_temporal_features(feature_data)\n",
    "        \n",
    "        # 2. TREND AND MOMENTUM FEATURES\n",
    "        print(\"   üìà Creating trend and momentum features...\")\n",
    "        feature_data = self.create_trend_features(feature_data)\n",
    "        \n",
    "        # 3. RELATIVE POSITION FEATURES\n",
    "        print(\"   üìä Creating relative position features...\")\n",
    "        feature_data = self.create_relative_features(feature_data)\n",
    "        \n",
    "        # 4. INTERACTION FEATURES\n",
    "        print(\"   üîÑ Creating interaction features...\")\n",
    "        feature_data = self.create_interaction_features(feature_data)\n",
    "        \n",
    "        # 5. LAGGED FEATURES\n",
    "        print(\"   ‚è™ Creating lagged features...\")\n",
    "        feature_data = self.create_lagged_features(feature_data)\n",
    "        \n",
    "        # Feature catalog\n",
    "        self.catalog_features(feature_data)\n",
    "        \n",
    "        return feature_data\n",
    "    \n",
    "    def create_temporal_features(self, data):\n",
    "        \"\"\"Create time-based features\"\"\"\n",
    "        \n",
    "        feature_data = data.copy()\n",
    "        \n",
    "        # Quarter and year\n",
    "        feature_data['quarter'] = feature_data.index.quarter\n",
    "        feature_data['year'] = feature_data.index.year\n",
    "        \n",
    "        # Economic cycle phases (simplified)\n",
    "        feature_data['is_recession_period'] = self.identify_recession_periods(feature_data)\n",
    "        \n",
    "        return feature_data\n",
    "    \n",
    "    def identify_recession_periods(self, data):\n",
    "        \"\"\"Identify recession periods based on economic indicators\"\"\"\n",
    "        \n",
    "        # Simplified recession indicator (in practice, use official NBER dates)\n",
    "        recession_flag = np.zeros(len(data))\n",
    "        \n",
    "        # Use unemployment spikes and GDP declines as proxies\n",
    "        if 'unemployment_rate' in data.columns:\n",
    "            unemployment_ma = data['unemployment_rate'].rolling(window=4).mean()\n",
    "            unemployment_spike = data['unemployment_rate'] > (unemployment_ma + 1.0)\n",
    "            recession_flag[unemployment_spike] = 1\n",
    "        \n",
    "        return recession_flag\n",
    "    \n",
    "    def create_trend_features(self, data):\n",
    "        \"\"\"Create trend and momentum features\"\"\"\n",
    "        \n",
    "        feature_data = data.copy()\n",
    "        \n",
    "        # Define economic series for trend calculations\n",
    "        economic_series = [col for col in data.columns if col not in ['quarter', 'year', 'is_recession_period']]\n",
    "        \n",
    "        for series in economic_series:\n",
    "            # Rolling means (short and long term)\n",
    "            feature_data[f'{series}_trend_3Q'] = data[series].rolling(window=3).mean()\n",
    "            feature_data[f'{series}_trend_8Q'] = data[series].rolling(window=8).mean()\n",
    "            \n",
    "            # Momentum (rate of change)\n",
    "            feature_data[f'{series}_momentum_1Q'] = data[series].pct_change(periods=1) * 100\n",
    "            feature_data[f'{series}_momentum_4Q'] = data[series].pct_change(periods=4) * 100\n",
    "            \n",
    "            # Volatility\n",
    "            feature_data[f'{series}_volatility_4Q'] = data[series].rolling(window=4).std()\n",
    "        \n",
    "        return feature_data\n",
    "    \n",
    "    def create_relative_features(self, data):\n",
    "        \"\"\"Create relative position and comparison features\"\"\"\n",
    "        \n",
    "        feature_data = data.copy()\n",
    "        \n",
    "        economic_series = [col for col in data.columns if col not in ['quarter', 'year', 'is_recession_period'] \n",
    "                         and '_trend_' not in col and '_momentum_' not in col and '_volatility_' not in col]\n",
    "        \n",
    "        for series in economic_series:\n",
    "            # Z-score normalization\n",
    "            mean_val = data[series].mean()\n",
    "            std_val = data[series].std()\n",
    "            feature_data[f'{series}_zscore'] = (data[series] - mean_val) / std_val\n",
    "            \n",
    "            # Percentile ranking\n",
    "            feature_data[f'{series}_percentile'] = data[series].rank(pct=True) * 100\n",
    "            \n",
    "            # Distance from historical extremes\n",
    "            max_val = data[series].max()\n",
    "            min_val = data[series].min()\n",
    "            feature_data[f'{series}_from_peak'] = (max_val - data[series]) / max_val * 100\n",
    "            feature_data[f'{series}_from_trough'] = (data[series] - min_val) / min_val * 100\n",
    "        \n",
    "        return feature_data\n",
    "    \n",
    "    def create_interaction_features(self, data):\n",
    "        \"\"\"Create interaction features between economic indicators\"\"\"\n",
    "        \n",
    "        feature_data = data.copy()\n",
    "        \n",
    "        # Key economic interactions for mortgage lending\n",
    "        if 'unemployment_rate' in data.columns and 'home_price_index' in data.columns:\n",
    "            feature_data['unemployment_housing_ratio'] = data['unemployment_rate'] / data['home_price_index']\n",
    "        \n",
    "        if 'mortgage_rates' in data.columns and 'income_level' in data.columns:\n",
    "            feature_data['affordability_index'] = data['income_level'] / data['mortgage_rates']\n",
    "        \n",
    "        if 'gdp_growth' in data.columns and 'inflation_rate' in data.columns:\n",
    "            feature_data['real_growth'] = data['gdp_growth'] - data['inflation_rate']\n",
    "        \n",
    "        return feature_data\n",
    "    \n",
    "    def create_lagged_features(self, data):\n",
    "        \"\"\"Create lagged features for predictive modeling\"\"\"\n",
    "        \n",
    "        feature_data = data.copy()\n",
    "        \n",
    "        # Define series for lagging\n",
    "        lag_series = [col for col in data.columns if col not in ['quarter', 'year', 'is_recession_period']]\n",
    "        \n",
    "        # Create multiple lags (1Q, 2Q, 4Q back)\n",
    "        lag_periods = [1, 2, 4]\n",
    "        \n",
    "        for series in lag_series:\n",
    "            for lag in lag_periods:\n",
    "                feature_data[f'{series}_lag_{lag}Q'] = data[series].shift(lag)\n",
    "        \n",
    "        return feature_data\n",
    "    \n",
    "    def catalog_features(self, feature_data):\n",
    "        \"\"\"Catalog and describe all engineered features\"\"\"\n",
    "        \n",
    "        feature_categories = {\n",
    "            'Temporal': ['quarter', 'year', 'is_recession_period'],\n",
    "            'Trend': [col for col in feature_data.columns if '_trend_' in col],\n",
    "            'Momentum': [col for col in feature_data.columns if '_momentum_' in col],\n",
    "            'Volatility': [col for col in feature_data.columns if '_volatility_' in col],\n",
    "            'Relative': [col for col in feature_data.columns if '_zscore' in col or '_percentile' in col or '_from_' in col],\n",
    "            'Interaction': ['unemployment_housing_ratio', 'affordability_index', 'real_growth'],\n",
    "            'Lagged': [col for col in feature_data.columns if '_lag_' in col]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìã FEATURE ENGINEERING SUMMARY:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        total_features = 0\n",
    "        for category, features in feature_categories.items():\n",
    "            count = len(features)\n",
    "            total_features += count\n",
    "            print(f\"   ‚Ä¢ {category:15} {count:3} features\")\n",
    "        \n",
    "        print(f\"\\n   üìä TOTAL FEATURES: {total_features}\")\n",
    "        \n",
    "        self.feature_catalog = feature_categories\n",
    "\n",
    "# Execute comprehensive feature engineering\n",
    "feature_engineer = FeatureEngineeringEngine()\n",
    "engineered_data = feature_engineer.create_comprehensive_features(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 6: FEATURE VALIDATION & BUSINESS ALIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ FEATURE VALIDATION ENGINE\n",
    "class FeatureValidationEngine:\n",
    "    \"\"\"\n",
    "    COMPREHENSIVE FEATURE VALIDATION AND BUSINESS ALIGNMENT\n",
    "    \n",
    "    Business Purpose: Ensure engineered features are statistically\n",
    "    sound and economically meaningful for mortgage lending context.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_results = {}\n",
    "    \n",
    "    def validate_feature_set(self, feature_data, target_series='approval_rate'):\n",
    "        \"\"\"\n",
    "        COMPREHENSIVE FEATURE VALIDATION\n",
    "        \n",
    "        Thinking: Validate features for modeling readiness and\n",
    "        business relevance before proceeding to modeling.\n",
    "        \"\"\"\n",
    "        print(\"\\n‚úÖ INITIATING COMPREHENSIVE FEATURE VALIDATION...\")\n",
    "        \n",
    "        validation_results = {}\n",
    "        \n",
    "        # 1. DATA QUALITY VALIDATION\n",
    "        print(\"   üîç Validating data quality...\")\n",
    "        validation_results['quality'] = self.validate_data_quality(feature_data)\n",
    "        \n",
    "        # 2. STATISTICAL PROPERTIES VALIDATION\n",
    "        print(\"   üìä Validating statistical properties...\")\n",
    "        validation_results['statistical'] = self.validate_statistical_properties(feature_data)\n",
    "        \n",
    "        # 3. BUSINESS RELEVANCE VALIDATION\n",
    "        print(\"   üíº Validating business relevance...\")\n",
    "        validation_results['business'] = self.validate_business_relevance(feature_data)\n",
    "        \n",
    "        # 4. FINAL ASSESSMENT\n",
    "        print(\"   üìã Generating final assessment...\")\n",
    "        self.generate_final_assessment(validation_results)\n",
    "        \n",
    "        self.validation_results = validation_results\n",
    "        return validation_results\n",
    "    \n",
    "    def validate_data_quality(self, data):\n",
    "        \"\"\"Validate feature data quality\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_counts = data.isna().sum()\n",
    "        total_missing = missing_counts.sum()\n",
    "        \n",
    "        results['missing_values'] = {\n",
    "            'total_missing': total_missing,\n",
    "            'complete_features': (missing_counts == 0).sum(),\n",
    "            'problematic_features': (missing_counts > len(data) * 0.1).sum()  # >10% missing\n",
    "        }\n",
    "        \n",
    "        # Check for infinite values\n",
    "        infinite_counts = np.isinf(data).sum().sum()\n",
    "        results['infinite_values'] = infinite_counts\n",
    "        \n",
    "        # Check for constant features\n",
    "        constant_features = []\n",
    "        for col in data.columns:\n",
    "            if data[col].nunique() <= 1:\n",
    "                constant_features.append(col)\n",
    "        \n",
    "        results['constant_features'] = constant_features\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def validate_statistical_properties(self, data):\
    "        \"\"\"Validate statistical properties of features\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Correlation analysis\n",
    "        correlation_matrix = data.corr()\n",
    "        \n",
    "        # Identify highly correlated features (>0.95)\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                if abs(correlation_matrix.iloc[i, j]) > 0.95:\n",
    "                    high_corr_pairs.append((\n",
    "                        correlation_matrix.columns[i], \n",
    "                        correlation_matrix.columns[j],\n",
    "                        correlation_matrix.iloc[i, j]\n",
    "                    ))\n",
    "        \n",
    "        results['high_correlation'] = high_corr_pairs\n",
    "        \n",
    "        # Feature variance\n",
    "        variances = data.var()\n",
    "        low_variance_features = variances[variances < 0.01].index.tolist()\n",
    "        results['low_variance_features'] = low_variance_features\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def validate_business_relevance(self, data):\n",
    "        \"\"\"Validate business relevance of features\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Check for economically meaningful features\n",
    "        expected_features = [\n",
    "            'unemployment_rate', 'mortgage_rates', 'home_price_index',\n",
    "            'gdp_growth', 'income_level', 'affordability_index'\n",
    "        ]\n",
    "        \n",
    "        missing_business_features = [f for f in expected_features if f not in data.columns]\n",
    "        results['missing_business_features'] = missing_business_features\n",
    "        \n",
    "        # Validate feature distributions make economic sense\n",
    "        distribution_checks = {}\n",
    "        \n",
    "        if 'unemployment_rate' in data.columns:\n",
    "            unemployment_range = (data['unemployment_rate'].min(), data['unemployment_rate'].max())\n",
    "            distribution_checks['unemployment_rate'] = {\n",
    "                'range': unemployment_range,\n",
    "                'plausible': 3 <= unemployment_range[0] <= 15 and 3 <= unemployment_range[1] <= 15\n",
    "            }\n",
    "        \n",
    "        if 'mortgage_rates' in data.columns:\n",
    "            mortgage_range = (data['mortgage_rates'].min(), data['mortgage_rates'].max())\n",
    "            distribution_checks['mortgage_rates'] = {\n",
    "                'range': mortgage_range,\n",
    "                'plausible': 2 <= mortgage_range[0] <= 12 and 2 <= mortgage_range[1] <= 12\n",
    "            }\n",
    "        \n",
    "        results['distribution_checks'] = distribution_checks\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_final_assessment(self, validation_results):\n",
    "        \"\"\"Generate final feature validation assessment\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìã FEATURE VALIDATION ASSESSMENT REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        quality = validation_results['quality']\n",
    "        statistical = validation_results['statistical']\n",
    "        business = validation_results['business']\n",
    "        \n",
    "        # Data Quality Assessment\n",
    "        print(f\"\\nüîç DATA QUALITY ASSESSMENT:\")\n",
    "        print(f\"   ‚Ä¢ Missing Values: {quality['missing_values']['total_missing']:,}\")\n",
    "        print(f\"   ‚Ä¢ Complete Features: {quality['missing_values']['complete_features']}\")\n",
    "        print(f\"   ‚Ä¢ Problematic Features: {quality['missing_values']['problematic_features']}\")\n",
    "        print(f\"   ‚Ä¢ Infinite Values: {quality['infinite_values']}\")\n",
    "        print(f\"   ‚Ä¢ Constant Features: {len(quality['constant_features'])}\")\n",
    "        \n",
    "        # Statistical Assessment\n",
    "        print(f\"\\nüìä STATISTICAL ASSESSMENT:\")\n",
    "        print(f\"   ‚Ä¢ Highly Correlated Feature Pairs: {len(statistical['high_correlation'])}\")\n",
    "        print(f\"   ‚Ä¢ Low Variance Features: {len(statistical['low_variance_features'])}\")\n",
    "        \n",
    "        # Business Relevance Assessment\n",
    "        print(f\"\\nüíº BUSINESS RELEVANCE ASSESSMENT:\")\n",
    "        print(f\"   ‚Ä¢ Missing Business Features: {len(business['missing_business_features'])}\")\n",
    "        \n",
    "        plausible_count = sum(1 for check in business['distribution_checks'].values() if check['plausible'])\n",
    "        total_checks = len(business['distribution_checks'])\n",
    "        print(f\"   ‚Ä¢ Plausible Value Ranges: {plausible_count}/{total_checks}\")\n",
    "        \n",
    "        # Overall Assessment\n",
    "        print(f\"\\n‚úÖ OVERALL ASSESSMENT:\")\n",
    "        \n",
    "        if (quality['missing_values']['total_missing'] == 0 and \n",
    "            quality['infinite_values'] == 0 and \n",
    "            len(quality['constant_features']) == 0 and\n",
    "            len(business['missing_business_features']) == 0 and\n",
    "            plausible_count == total_checks):\n",
    "            print(\"   üü¢ EXCELLENT: Features ready for modeling\")\n",
    "        elif (quality['missing_values']['total_missing'] < 100 and \n",
    "              quality['infinite_values'] == 0 and\n",
    "              len(business['missing_business_features']) <= 2):\n",
    "            print(\"   üü° GOOD: Features suitable for modeling with minor notes\")\n",
    "        else:\n",
    "            print(\"   üî¥ ATTENTION NEEDED: Review feature quality before modeling\")\n",
    "\n",
    "# Execute comprehensive feature validation\n",
    "validator = FeatureValidationEngine()\n",
    "validation_results = validator.validate_feature_set(engineered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 7: DATASET EXPORT & DOCUMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ ENTERPRISE DATA EXPORT ENGINE\n",
    "class DataExportEngine:\n",
    "    \"\"\"\n",
    "    PROFESSIONAL DATA EXPORT AND DOCUMENTATION ENGINE\n",
    "    \n",
    "    Business Purpose: Export cleaned, engineered data with\n",
    "    comprehensive documentation for modeling and business use.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.export_log = []\n",
    "    \n",
    "    def export_final_datasets(self, feature_data, validation_results, feature_catalog):\n",
    "        \"\"\"\n",
    "        EXPORT COMPREHENSIVE DATASET SUITE\n",
    "        \n",
    "        Thinking: Create multiple dataset versions for different\n",
    "        use cases with full documentation.\n",
    "        \"\"\"\n",
    "        print(\"\\nüíæ INITIATING COMPREHENSIVE DATA EXPORT...\")\n",
    "        \n",
    "        import os\n",
    "        import json\n",
    "        from datetime import datetime\n",
    "        \n",
    "        # Create export directory structure\n",
    "        os.makedirs('../data/cleaned', exist_ok=True)\n",
    "        os.makedirs('../data/modeling', exist_ok=True)\n",
    "        os.makedirs('../data/documentation', exist_ok=True)\n",
    "        \n",
    "        # Create timestamp for versioning\n",
    "        export_timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "        \n",
    "        # 1. EXPORT CLEANED DATASET\n",
    "        print(\"   üìÇ Exporting cleaned dataset...\")\n",
    "        self.export_cleaned_dataset(feature_data, export_timestamp)\n",
    "        \n",
    "        # 2. EXPORT MODELING DATASET\n",
    "        print(\"   üîß Exporting modeling dataset...\")\n",
    "        self.export_modeling_dataset(feature_data, export_timestamp)\n",
    "        \n",
    "        # 3. CREATE COMPREHENSIVE DOCUMENTATION\n",
    "        print(\"   üìã Creating documentation...\")\n",
    "        self.create_documentation(feature_data, validation_results, feature_catalog, export_timestamp)\n",
    "        \n",
    "        # 4. EXPORT SUMMARY REPORT\n",
    "        print(\"   üìä Generating summary report...\")\n",
    "        self.generate_summary_report(feature_data, export_timestamp)\n",
    "    \n",
    "    def export_cleaned_dataset(self, data, timestamp):\n",
    "        \"\"\"Export the fully cleaned and engineered dataset\"\"\"\n",
    "        \n",
    "        # Export to multiple formats for different use cases\n",
    "        data.to_parquet(f'../data/cleaned/cleaned_economic_data_{timestamp}.parquet')\n",
    "        data.to_parquet('../data/cleaned/current_cleaned_economic_data.parquet')\n",
    "        \n",
    "        # Export CSV for business users\n",
    "        data.to_csv(f'../data/cleaned/cleaned_economic_data_{timestamp}.csv')\n",
    "        \n",
    "        print(f\"      ‚Ä¢ Parquet files created (version: {timestamp})\")\n",
    "        print(\"      ‚Ä¢ CSV file created for business users\")\n",
    "    \n",
    "    def export_modeling_dataset(self, data, timestamp):\n",
    "        \"\"\"Export dataset optimized for modeling\"\"\"\n",
    "        \n",
    "        # Remove highly correlated features identified in validation\n",
    "        modeling_data = data.copy()\n",
    "        \n",
    "        # Keep only the most recent lag for each series to reduce multicollinearity\n",
    "        lag_columns_to_remove = [col for col in data.columns if '_lag_2Q' in col or '_lag_4Q' in col]\n",
    "        modeling_data = modeling_data.drop(columns=lag_columns_to_remove)\n",
    "        \n",
    "        # Export modeling dataset\n",
    "        modeling_data.to_parquet(f'../data/modeling/modeling_economic_data_{timestamp}.parquet')\n",
    "        modeling_data.to_parquet('../data/modeling/current_modeling_economic_data.parquet')\n",
    "        \n",
    "        print(f\"      ‚Ä¢ Modeling dataset created: {len(modeling_data.columns)} features\")\n",
    "        print(f\"      ‚Ä¢ Removed {len(lag_columns_to_remove)} highly correlated lag features\")\n",
    "    \n",
    "    def create_documentation(self, data, validation_results, feature_catalog, timestamp):\n",
    "        \"\"\"Create comprehensive dataset documentation\"\"\"\n",
    "        \n",
    "        documentation = {\n",
    "            'dataset_info': {\n",
    "                'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'version': timestamp,\n",
    "                'total_features': len(data.columns),\n",
    "                'time_period': f\"{data.index.min().strftime('%Y-%m-%d')} to {data.index.max().strftime('%Y-%m-%d')}\",\n",
    "                'total_observations': len(data)\n",
    "            },\n",
    "            'feature_catalog': feature_catalog,\n",
    "            'validation_results': {\n",
    "                'data_quality': validation_results['quality'],\n",
    "                'business_relevance': validation_results['business']\n",
    "            },\n",
    "            'data_sources': {\n",
    "                'original_sources': 'Federal Reserve, BLS, Census Bureau, Freddie Mac',\n",
    "                'collection_method': 'API integration and manual collection',\n",
    "                'update_frequency': 'Quarterly'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save documentation\n",
    "        with open(f'../data/documentation/dataset_documentation_{timestamp}.json', 'w') as f:\n",
    "            json.dump(documentation, f, indent=2)\n",
    "        \n",
    "        with open('../data/documentation/current_dataset_documentation.json', 'w') as f:\n",
    "            json.dump(documentation, f, indent=2)\n",
    "        \n",
    "        print(\"      ‚Ä¢ Comprehensive documentation created\")\n",
    "    \n",
    "    def generate_summary_report(self, data, timestamp):\n",
    "        \"\"\"Generate business-focused summary report\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìà DATA CLEANING & FEATURE ENGINEERING SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\n‚úÖ PROCESSING COMPLETE:\")\n",
    "        print(f\"   ‚Ä¢ Final Dataset: {len(data.columns)} engineered features\")\n",
    "        print(f\"   ‚Ä¢ Time Coverage: {data.index.min().strftime('%Y-%m')} to {data.index.max().strftime('%Y-%m')}\")\n",
    "        print(f\"   ‚Ä¢ Data Quality: Zero missing values, ready for modeling\")\n",
    "        \n",
    "        print(f\"\\nüìÅ EXPORTED FILES:\")\n",
    "        print(f\"   ‚Ä¢ Cleaned Data: ../data/cleaned/current_cleaned_economic_data.parquet\")\n",
    "        print(f\"   ‚Ä¢ Modeling Data: ../data/modeling/current_modeling_economic_data.parquet\")\n",
    "        print(f\"   ‚Ä¢ Documentation: ../data/documentation/current_dataset_documentation.json\")\n",
    "        \n",
    "        print(f\"\\nüéØ NEXT STEPS:\")\n",
    "        print(f\"   1. Exploratory Data Analysis ‚û°Ô∏è Notebook 3\")\n",
    "        print(f\"   2. Predictive Model Development ‚û°Ô∏è Notebook 4\")\n",
    "        \n",
    "        print(f\"\\nüí° READINESS ASSESSMENT: üü¢ READY FOR ANALYSIS\")\n",
    "\n",
    "# Execute comprehensive data export\n",
    "exporter = DataExportEngine()\n",
    "exporter.export_final_datasets(engineered_data, validation_results, feature_engineer.feature_catalog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}